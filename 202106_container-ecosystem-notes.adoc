= The Container Ecosystem
Thomas SCHWENDER <https://github.com/ardemius[@ardemius]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:source-highlighter: highlightjs
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 4
// To number the sections of the table of contents
:sectnums:
// To turn off figure caption labels and numbers
:figure-caption!:
// Same for examples
//:example-caption!:
// To turn off ALL captions
// :caption:

toc::[]

== Overview

Talk ayant pour but de retracer l'histoire de la conteneurisation, depuis ses débuts jusqu'à nos jours, et d'expliquer les raisons derrière chaque grande évolution.
Nous commencerons par les prémises avec les cgroups et namespaces, puis parlerons de l'émergence de Docker, son évolution et l'arrivée des low level (ex : runc) et high level (ex : containerD) runtimes, et finiront avec les alternatives actuelles à Docker, permettant de gérer complètement le cycle de vie d'un container sans ce dernier.

== Recherche du nom de la conférence

* A brief history of containerization, from yesterday to today
* Histoire accélérée de la conteneurisation, d'hier à nos jours

== Ressources

* A : https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : excellente ressource
    ** Jeter un oeil au site parent, qui est vraiment très bien : https://www.tutorialworks.com/
    ** Attention, les schémas ne font pas suffisamment apparaître le *Docker Daemon* (*dockerd*) selon moi

* B : https://blog.engineering.publicissapient.fr/2019/12/23/docker-est-mort-vive-docker/
    ** Dans les 1eres minutes, le Docker Daemon est mieux présenté
        *** reprendre le schéma en 2:01, il est complet AVEC le docker daemon
            **** pour un schéma complet, voir également : http://sysblog.informatique.univ-paris-diderot.fr/wp-content/uploads/2020/03/Docker-2.3.png
        *** on pourrait également faire apparaître le *docker registry* sur le schéma

    ** Très bonne présentation des différents éléments de "Docker", qui est un fork de *Moby*
        *** https://mobyproject.org/ : Moby is an open framework created by Docker to assemble specialized container systems without reinventing the wheel.
            **** Moby permet de pratiquer avec la plomberie de Docker "Docker internals", il n'est pas conseillé si l'on souhaite simplement un moyen simple et rapide de lancer des containers

* Schéma de Docker en 2019 (récent) : https://www.codetd.com/en/article/6502770
    ** montre les 3 parties de *Docker engine*, à savoir : Docker Daemon (dockerd), ContainerD, RunC
        *** NON ! Préférer l'explication fournie plus bas : +
        Docker Engine = Docker Server (implémenté à l'aide de dockerd, qui lui même utilise containerd, qui lui même utilise runc) + API + CLI
    ** pour des définitions de *ContainerD* et *RunC*, voir https://jfrog.com/knowledge-base/the-basics-7-alternatives-to-docker-all-in-one-solutions-and-standalone-container-tools/
        *** voir également https://docs.docker.com/engine/api/, où il est écrit : +
            "Docker provides an API for interacting with the Docker daemon (called the Docker Engine API), as well as SDKs for Go and Python"
        *** NON ! Plus clair, site même de Docker : https://docs.docker.com/engine/ : 
+
----
Docker Engine acts as a client-server application with:

- A server with a long-running daemon process dockerd.
- APIs which specify interfaces that programs can use to talk to and instruct the Docker daemon.
- A command line interface (CLI) client docker.
----
            **** A l'aide de cette dernière explication, on se rend compte que *Docker Engine* regroupe en fait la *CLI*, la Docker Engine *API* et le *Docker daemon*. +
            Ce dernier est peut-être considéré ici comme *"englobant" containerd et runc*, étant donné que le *schéma d'architecture* https://docs.docker.com/get-started/overview/#docker-architecture montre le docker daemon en lien avec la gestion des images, elle-même liée aux containers
            **** concernant la *Docker Engine API* permettant l'interaction avec le Docker Daemon, voir https://docs.docker.com/engine/api/

    ** autre bon schéma : https://www.aquasec.com/cloud-native-academy/docker-container/docker-architecture/ +
    Ce dernier indique également que le Docker Engine englobe la CLI, l'API de comm avec le docker daemon, et le docker daemon lui-même +
    PAR CONTRE, est-ce toujours totalement d'actualité ? Aucune mention à runc et containerd, ce qui me pose un petit problème...
        *** OUI, c'est bien toujours d'actualité. Vu plus bas, le docker server (implémenté à l'aide de docker daemon) contient bien / utilise bien containerd et runc.
    ** réponse finale ici : https://www.studytrails.com/2018/12/04/docker-architecture-engine-containerd-runc/ +
    *Docker Engine* est bien composé de : 
        *** *Docker Server*, qui est implémenté à l'aide de *docker daemon (dockerd)*, et qui est responsable de la création des images, containers, networks et volumes
            **** Et on considère que le *Docker Server contient containerd et runc*
        *** a *RESTFul API* to talk to the docker server -> donc une API pour parler à dockerd, c'est à dire *Docker Engine API*
        *** une *CLI* (the docker command)
    
    ** *dockerd* is the thing that helps you *work with volumes*, *networking* or even *orchestration*. +
    And of course it *can launch containers* or *manage images* as well, *but containerd is listening on linux socket* and this is *just translated to calls to its GRPC API*. +
    see https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e

    ** Une bonne comparaison, rapide et efficace entre Docker et Kubernetes : (https://www.threatstack.com/blog/diving-deeper-into-runtimes-kubernetes-cri-and-shims) +
    "*Docker* is a technology for automating the process of deploying containers. *Kubernetes* is orchestration software that gives us an API to manage how the containers will run." +
    "In a broad sense, Docker runs on nodes, and Kubernetes runs clusters of nodes. To run containers in pods, Kubernetes uses runtimes. Considering what we know about runtimes and how they are defined, Docker can be considered a runtime for Kubernetes, and is a high-level runtime as defined in our last post."

    ** On pourrait également définir Docker très simplement ainsi : *Docker allows to run containerized apps*
        *** Au final, les composants de Docker ont pour but de : *build des images*, et *run des containers*

* dockerd vs containerd vs runc : https://stackoverflow.com/questions/46649592/dockerd-vs-docker-containerd-vs-docker-runc-vs-docker-containerd-ctr-vs-docker-c
    ** on y trouve aussi une bonne explication sur *shim* : +
    "(docker-)containerd-shim - After runC actually runs the container, it exits (allowing us to not have any long-running processes responsible for our containers). The shim is the component which sits between containerd and runc to facilitate this."

    ** toujours concernant shim (docker-containerd-shim), voir pour une bonne explication : https://www.threatstack.com/blog/diving-deeper-into-runtimes-kubernetes-cri-and-shims +
    Le point essentiel de shim est de permettre "It allows for *daemon-less containers*." +
    "It basically sits as the parent of the container’s processes to facilitate communications, and eliminates the long running runtime processes for containers." +
    "The processes of the *shim and the container* are bound tightly; however, they are *totally separated from the process of the container manager*" +
    "Shim allows a runtime (runC) to exit after the container is started. Without this we would still be subject to long runtime processes."
        *** cet article décrit également très bien Kubernetes et Docker, et les liens entre Kubelet, implémentation de CRI (CRI-O) et un low-level container runtime (très souvent runc)
    ** autre bon article sur le sujet : https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e
        *** *containerd-shim* is the *parent process of every container started* and it *also allows daemon-less containers* (meaning you can upgrade docker daemon without restarting all your containers, which was a big pain)

    ** *dockershim* est également très bien expliqué dans https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : +
    "In tech terms, a shim is a component in a software system, which acts as a *bridge between different APIs*, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work."

* autre *très bonne ressource*, la série d'articles de *Ian Lewis* (2017/12) : https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
    ** en fait, toutes les différentes facettes de l'écosystème des containers y sont présentées (docker, dockerd, containerd, runc)
    ** et une fois lu, voir également https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e

* pour une explication de ce qui a amené aux containers, avec les *namespaces*, les *cgroups* (control groups), l'isolation des appels (*seccomp-bpf*), et finalement les "containers Docker", voir l'excellent article https://jvns.ca/blog/2016/10/10/what-even-is-a-container/
    ** Docker a fourni un wrapping simple et facile d'utilisation de ces fonctionnalités du kernel Linux (et en a également apporté d'autres également)
    ** A l'occasion, regarder le Zine "How Containers work" de Julia Evans (2020) : https://wizardzines.gumroad.com/l/containers-zine/buyonegiveone / https://jvns.ca/blog/2020/04/27/new-zine-how-containers-work/
        *** Ce Zine contient une description sympa des *container Kernel features* : 
            **** *pivot_root* : set a process's root directory to a directory with the contents of the container image
                ***** difference between pivot_root and *chroot* : chroot is easy to escape from if you're root and pivot root isn't +
                -> so containers use pivot_root instead of chroot
            **** *cgroups* : limit memory / CPU usage for a group of processes
            **** *namespaces* : allow processes to have their own network / PIDs / users / hostname / mounts / and more !
            **** *seccomp-bpf* : security: prevent dangerous system calls
                ***** seccomp means "secure computing"
                ***** bpf, pour Berkeley Packet Filter, est une extension de seccomp
            **** *capabilities* : security: avoid giving root access +
            Capabilities allow to reduce the privileges of an active process
            **** *overlay filesystems* : optimization to reduce disk space used by containers which are using the same image
            **** quand on utilise toutes les fonctionnalités précédentes, on a un container
            **** Et un GROS reminder : *A container is a group of processes*

    ** Cf wikipedia (https://en.wikipedia.org/wiki/Cgroups), *cgroups* : +
    "cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes."
        *** la vidéo https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de Jérôme PETAZZONI (Docker) explique en détails les différentes fonctionnalités des *cgroups*, *différents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            **** Il est également question des *container runtimes* qui sont basés sur les cgroups et les namespaces. +
            Exemples de container runtimes basés sur des namespaces et des cgroups : 
                ***** *LXC* : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                ***** *systemd-nspawn*
                ***** *Docker*
                ***** *rkt*
                ***** *runC*
                ***** All those container runtimes use the same kernel features (at that time, 2015 ?)
            **** et maintenant des container runtimes qui ne sont PAS basés sur les namespaces et les cgroups : 
                **** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                **** *Jails* / *Zones*
            **** la vidéo de Jérôme se termine par un live demo d'une création de container *à la main* (un début de container)
            **** autre très bonne vidéo de container complètement créé à la main en Go, https://www.youtube.com/watch?v=Utf-A4rODH8, de *Liz RICE* (2016/10)
                **** Voir également le Gist en GO de *Julien Friedman* dont Liz s'est inspirée : https://gist.github.com/julz/c0017fa7a40de0543001 (au final on build un container en ~55 lignes de Go)
        *** le travail sur les cgroups a commencé en 2006 chez Google sous le nom "process containers", avant d'être renommé en "control groups" pour éviter toute confusion avec le terme "container" dans un contexte Linux Kernel

    ** *namespaces* are a Linux feature allowing your processes to be separated from the other processes on the computer. +
    You can have PID namespace, networking namespace, mount namespace. +
    Namespaces can be creates using the `unshare` program.

Une bonne définition d'un *container runtime* : +
.https://www.quora.com/What-is-container-runtime-in-Kubernetes/answer/John-Sundarraj
----
A container runtime is a library or software which has the ability to create, deploy and manage containers on its own. Basically, container runtimes are responsible for container lifecycle. It provides simple API layer to create, deploy and manage containers.
----

* *Docker was released in 2013*

* *Why Docker ?* by Solomon Hykes : https://www.youtube.com/watch?v=3N3n9FzebAA (2013/08/01, EXCELLENTE conf, toujours d'actualité) +
La grande raison de l'époque : *shipping software from A to B, reliably and automatically*
    ** It has to behave the same way on both machine, and this with technological stack behind applications being more and more complex
    ** and your shipping place can be different depending on developer environment, servers, etc etc. (a lot of possible combinations that result finally in different environments)
    ** 08:39 (https://youtu.be/3N3n9FzebAA?t=519), to avoid all those shipping problems in the (shipping) industry, one day in the 1950s, people agreed on using a standard box, with standard dimensions, weight, way to open the doors, etc etc. AND it resulted with the creation on the container we know today. +
    This "ugly box" allows *separation of concerns* : je crée un outil / soft, je veux le shipper, je le mets dans le container, et ma responsabilité pour le shipping s'arrête là. Je ne m'intéresse QU'A mon produit, et PAS au container. +
    De la même façon, pour les personnes en charge du shipping, elles n'ont pas besoin de s'intéresser à ce qu'il y a dans le container : elles savent que le container a une taille, un poids, des dimensions données, et que TOUS ces containers peuvent être utilisés via les mêmes moyens standards.
        *** ces "boîtes" ont réellement changé le monde à cette époque : AVANT, c'était une galère de livrer du fait de toutes les combinaisons possibles de packaging des produits à livrer.
        *** We finally wanted to do the same in our IT world for our own shipping needs.
        
    ** Avant, on avait bien déjà des archives comme des jars, rvms, etc. MAIS ce *sandboxing n'était pas complet*

    ** Il y avait bien *les VMs* : cette fois-ci, on a l'appli et on livre finalement toute la machine avec. On est maintenant sûr qu'on a bien le même "contexte" à chaque livraison.
        *** C'est la seule façon de s'assurer de share software in a truly reliable and repeatable way : to *ship the WHOLE system with the application* (because, truly, the system is PART OF the application)
        *** *le souci* avec les VMs est que l'*on ship trop de choses* : hard drives, network interfaces, le total de RAM, le type de processeur, etc. 
            **** Et il ne faut pas que ce soit le développeur qui décide comment l'on va faire fonctionner son application sur toutes les infrastructures possibles, ce n'est pas son rôle (on brise la "separation of concerns" précédente)
                ***** Pour reprendre l'analogie avec les "vrais" containers, cela reviendrait à imposer le modèle de grue avec lequel les décharger, et le modèle de bateau avec lequel les transporter.
                ***** In our IT world, the infrastructure provider is NOT free to make those choices just because you give them to him with your application.
        *** autre souci, *les VMs sont volumineuses* : est-ce facile d'en faire tourner 10 en parallèle ? Non.
            **** En fait, les VMs ont certains des "défauts" des machines classiques : elles mettent du temps à booter, consomment beaucoup de RAM, etc etc. Pas le plus pratique pour un dev dans son travail quotidien.
        
    ** Pour avoir le *meilleur des 2 mondes*, archives et VMs, il faudrait : 
        *** Sandbox the entire system
        *** without machine details
        *** and without the performance hit
        *** Et tout ceci est rendu *possible grâce aux fonctionnalités du kernel Linux*, tout particulièrement le *namespacing* qui a été rendu "réellement" fonctionnel dernièrement
            **** avec ce nouveau namespacing (2013), on peut maintenant isoler n'importe quel process des autres, et faire "croire" à ce process qu'il a sa propre VM (alors qu'il ne l'a pas)
                ***** mais utiliser ces fonctionnalités d'isolation du kernel Linux n'est pas évident, ce qu'il manque est une façon standard de les utiliser (un container standard pour cela) : c'est ce qu'est Docker +
                Docker est avant tout : 
                ***** un standard container format
                ***** simple tools that enable people running the infrastructure to take that container (without knowing what is inside), and then run it


* https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r

    ** developers who want to run apps in containers will need more than just the features that low-level runtimes provide, they need APIs and features around image formats, image management, and sharing images, which are provided by high-level runtimes.
    ** Developers who implement low-level runtimes will say that higher level runtimes like *containerd* and *cri-o* are not actually container runtimes, as from their perspective they outsource the implementation of running a container to *runc*.

* https://www.ianlewis.org/en/container-runtimes-part-2-anatomy-low-level-contai : *LOW LEVEL CONTAINER RUNTIME*

    ** le concept de *low-level container runtime* est mis en avant
    ** Low-level runtimes have a limited feature set and typically perform the low-level tasks for *running a container* (ex : runC)
        ** low-level runtimes are responsible for the mechanics of actually running a container
        ** raison pour laquelle de nombreux low-level container runtime s'appellent "run<quelque chose>"
    ** *Namespaces* let you virtualize system resources, like the file system or networking for each container.
        *** Namespaces are "what you can see"
    ** *cgroups* provide a way to limit the amount of resources, such as CPU and memory, that each container can use.
        *** control groups are "what you can use"
    ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.
    ** Examples of low-level container runtimes : 

        *** *lmctfy* (Let Me Contain That For You) : projet by Google, based on the internal container runtime that *Borg* uses. +
        It supports container hierarchies that use cgroup hierarchies via the container names (a root container called "busybox" could create sub-containers under the name "busybox/sub1" or "busybox/sub2") +
        While lmctfy provides some interesting features and ideas, other runtimes were more usable so Google decided it would be better for the community to focus worked on Docker's libcontainer instead of lmctfy.
            *** *Borg* is Google's cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. +
            See https://research.google/pubs/pub43438/ for more details

        *** *runC* : most widely used container runtime
            **** originally developed as part of Docker, then extracted as a separate tool and library.
                ***** So runC is the low-level runtime that was broken off from Docker.
            **** runC implements the *OCI runtime spec* (Open Container Initiative)
                ***** Pour plus détails, lire l'OCI runtime spec : https://github.com/opencontainers/runtime-spec
            **** https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : runc is responsible for creating and running the container process.

        *** *rkt* (CoreOS *Rocket*):
            **** developed by CoreOS, which was later acquired by Red Hat
            **** provides all features provided by low-level container runtimes, PLUS some high-level ones
            **** *projet ended / discontinued on 2020/02* and is not maintained anymore.
                ***** for more details on the reasons, see https://github.com/rkt/rkt/issues/4024 +
                The main ones seem to be : 
                ***** the previous development team at CoreOS got dismantled, and post Red Hat acquisition there are no plan to push the development forward
                ***** no more have development plans for rkt (from the new development team)
                ***** a declining engagement from the community

* https://www.ianlewis.org/en/container-runtimes-part-3-high-level-runtimes : *HIGH LEVEL CONTAINER RUNTIMES*

    ** *high-level runtimes* are responsible for *transport and management of container images*, unpacking the image, and *passing off to the low-level runtime* to *run the container*.
    ** Typically, high-level runtimes provide a *daemon* application and an *API* that remote applications can use to logically run containers and monitor them but they sit on top of and *delegate to low-level runtimes* or other high-level runtimes for the actual work. +
    High-level runtimes can also provide *features* that sound low-level, but are *used across individual containers on a machine*. For example, one feature might be the management of network namespaces, and allowing containers to join another container's network namespace.
    ** Exemples of high-level container runtime : 

        *** *Docker*
            **** Originally built as a monolithic daemon, *dockerd*, and the *docker client (Docker CLI)* application. +
            The daemon provided most of the logic of building containers, managing the images, and running containers, along with an API. +
            The command line client could be run to send commands and to get information from the daemon.
            **** It really was *the first* popular runtime to incorporate all of the features needed during the lifecycle of building and running containers, hence its success.
            **** A la base Docker faisait tout, les low et les high level features, mais cela a depuis (v1.11) été scindé en différentes briques, dont containerd et runC. +
            Docker se compose donc maintenant (2021) de docker CLI, dockerd, docker-containerd et docker-runc (les 2 derniers étant simplement des versions packagées de containerd et runc) ainsi que la Docker Engine API
                ***** *dockerd* provides features such as *building images*, and dockerd uses docker-containerd to provide features such as image management and running containers. For instance, Docker's build step is actually just some logic that interprets a Dockerfile, runs the necessary commands in a container using containerd, and *saves the resulting container file system as an image*.

        *** *ContainerD* 
            **** is the high-level runtime that was split off from Docker.
            **** implements downloading images, managing them, and running containers from images. +
            When it needs to *run a container* it unpacks the image into an OCI runtime bundle and *shells out to runc* to run it.
            **** Containerd also provides an API and client application that can be used to interact with it. The *containerd command line client* is *ctr*.
            ****  In contrast with Docker, containerd is *focused solely on running containers*, so it *does NOT provide a mechanism for building containers*.
                ***** Docker was focused on end-user and developer use cases, whereas containerd is focused on operational use cases, such as running containers on servers. Tasks such as building container images are left to other tools.
                ***** traduction simple : containerd can't build images (c'est le travail du daemon dockerd par exemple)
            **** containerd is made *compliant with CRI* through its *CRI plugin* "cri-containerd" (as coming from Docker, it is NOT natively compliant with CRI which comes from Kubernetes)
                ***** see https://github.com/containerd/cri for more details

        *** *rkt*
            **** CAREFUL ! See above, *projet ended in 2020/02* !
            **** rkt is a runtime that has both low-level and high-level features
            **** rkt allows you to *build container images*, *fetch* and *manage container images* in a local repository, and *run them* all from a single command

* https://www.ianlewis.org/en/container-runtimes-part-4-kubernetes-container-run : *KUBERNETES CONTAINER RUNTIMES & CRI*

    ** *Kubernetes* runtimes are *high-level container runtimes* that support the Container Runtime Interface (*CRI*) (mandatory to integrate with Kubernetes)

        *** CRI was introduced in Kubernetes 1.5 and acts as a *bridge* between the *kubelet* and the *container runtime*.
            **** *kubelet* : https://kubernetes.io/docs/concepts/overview/components/#kubelet +
            "An *agent* that runs on each node in the cluster. It *makes sure that containers are running in a Pod*. +
            The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The *kubelet doesn't manage containers which were not created by Kubernetes*"
            **** The kubelet is responsible for managing the container workloads for its node. +
            When it comes to actually run the workload, the kubelet uses CRI to communicate with the container runtime running on that same node. +
            In this way *CRI is simply an abstraction layer* or API that allows you to switch out container runtime implementations instead of having them built into the kubelet.
                ***** *CRI évite donc de coupler kubelet avec le container runtime* (logique, c'est une interface)

    ** The runtime is expected to handle the *management of images* and to *support Kubernetes pods*, as well as *manage the individual containers*. As a consequence, a Kubernetes runtime must be a high-level runtime per our definition in part 3.

    ** *containerd*
        *** implements CRI as a plugin, which is enabled by default
        *** it *supports multiple low-level runtimes* via something called a "runtime handler" starting in version 1.2. The runtime handler is passed via a field in CRI and based on that runtime handler containerd runs an application called a *shim* to start the container. This can be used to run containers using low-level runtimes other than runc, like *gVisor*, *Kata Containers*, or *Nabla Containers*.
            **** *gVisor*, *Kata Containers* et *Nabla Containers* sont souvent comparés car mettant tous en avant une *isolation très forte vis à vis de l'host*
            **** https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e : +
            kata containers "is claiming to be all the isolation you love from VMs but that can be easily plugged into all the tooling we have around containers. This means you can spin up these VMs (or kata containers if you wish) through docker or Kubernetes."

    ** *Docker*
        *** Nowadays, Docker itself isn't necessary to support CRI, which is done through the use of containerd

    ** *cri-o*
        *** cri-o is a lightweight *CRI runtime* made as a *Kubernetes specific high-level runtime*.
        *** It supports the management of OCI compatible images and pulls from any OCI compatible image registry.
        *** It *supports runc* and *Clear Containers* as low-level runtimes. +
        It supports other OCI compatible low-level runtimes in theory, but relies on compatibility with the runc OCI command line interface, so in practice it isn't as flexible as containerd's shim API.

    ** the *CRI Specification*
        *** CRI is a *protocol buffers* and *gRPC* API.
        *** CRI *defines several remote procedure calls* (RPCs) and *message types*. The RPCs are for operations like "pull image" (ImageService.PullImage), "create pod" (RuntimeService.RunPodSandbox), "create container" (RuntimeService.CreateContainer), "start container" (RuntimeService.StartContainer), "stop container" (RuntimeService.StopContainer), etc.
        *** We can interact with a CRI runtime directly using the crictl tool. crictl lets us send gRPC messages to a CRI runtime directly from the command line.

*OCI* : *Image spec* ET *Runtime spec*

    * cf "https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/" : the Open Container Initiative (OCI) which publishes specifications for images and containers.
        *** cf https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426, il est bien question de specifications pour des image-spec et runtime-spec
            **** Dans le schéma de https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/, il est expliqué que : +
            "OCI provides specifications for container images and running containers."
    * "https://blog.engineering.publicissapient.fr/2019/12/23/docker-est-mort-vive-docker/" voir en 2:06
    * runc est une implémentation de OCI

* Attention ! Fin 2020 (décembre) *deprecation de docker/docker-shim* (dockershim)
    ** oui, c'est bien confirmé : "the Kubernetes community announced it is deprecating Docker as a container runtime after v1.20". +
    Donc, il s'agit bien de la deprecation de *docker-shim*, ET *NON* de containerd-shim, qui n'a rien à voir sinon le "shim" dans le nom. +
    "Docker-shim was a temporary solution proposed by the Kubernetes community to add support for Docker so that it could serve as its container runtime." +
    Pour plus de détails, voir https://kubesphere.io/blogs/dockershim-out-of-kubernetes/ et https://linoxide.com/docker-alternative-container-tools/

* Attention ! 2021/09, changement de licence Docker Desktop, on ne peut plus l'utiliser sur Windows en entreprise.
* Parler de Docker Desktop qui conseille maintenant de passer, avec WSL 2, aux Linux Containers ?

* Une présentation de Podman, à Devoxx France 2021 (2021/10), par Benjamin Vouillaume : https://www.youtube.com/watch?v=pUFIG2AMDhg

== Plan du talk

Commencez par un petit disclaimer pour le public ? "ce que ce talk n'est pas ?" (à savoir une prez sur comment utiliser Docker, CRI-O ou Podman)

1. L'histoire de la containerisation d'hier à aujourd'hui

    * cgroups et namespaces : les premisses des containers
    * Puis Docker
        ** prévoir schéma de ce "Docker l'ancien"
    * Puis Docker 1.11 et le "split" avec containerd et runc
        ** nouveau schéma, Docker Engine, server avec containerd et runc
        ** checker où en était Kubernetes à date du split (2016/04). CRI avait-il déjà fait son apparition ?
    * Puis l'arrivée de Kubernetes qui a entraîné l'apparition de CRI
        ** nouveau schéma avec Docker ET Kubernetes, et leurs container runtimes
    * Et maintenant toutes les alternatives possibles à "Docker" (en fait, de nouveaux high et low container runtime)
        ** pour les alternatives, voir https://linoxide.com/docker-alternative-container-tools/
        ** Podman (grâce à Kubernetes) est devenu un incontournable

    Ou on commence par une frise temporelle du début des containers à nos jours ?

    A chaque début de nouvelle section, reprendre où nous en sommes dans la frise temporelle

2. *Les cgroups et les namespaces*

    ** pourquoi a-t-on fait ça ? Principalement pour des besoins d'isolation
        *** retrouver les 1ers usages

3. *L'arrivée de Docker, le début des containers*

    ** et au début, les containers, pour l'immense majorité des devs, c'était Docker et rien que Docker.
        *** Maintenant, il y a Docker la compagnie, et Docker la technologie
    ** Docker la compagnie ? Les images, les containers, la ligne de commande ?
    ** A la base la compagnie Docker a créé un outil simple et ergonomique pour travailler avec les containers, outil appelé "docker" (la CLI docker pour être plus précis)
        *** cette CLI permet très facilement to build images, pull them from registries, create, start and manager containers
    ** et la grosse différence se fait avec le passage à la version 1.11, et l'apparition de containerd et runc

    ** *Docker "à l'ancienne" avant la 1.11 (2016/04)*
        *** https://jvns.ca/blog/2016/10/02/i-just-want-to-run-a-container/

4. *Le split de Docker : l'apparition de containerd et runc*

    On a maintenant le Docker engine
    ** Docker client (CLI, GUI, etc.) 
    ** parle à un Docker Daemon 
    ** qui parle à containerd : un autre daemon qui va aller surveiller vos containers, les redémarrer
        *** containerd supervise les containers (start, stop, pause)
    ** qui à runc : une librairie, un espèce de wrapper qui va vous permettre de lancer plus facilement des processus isolés
        *** et c'est runc qui va lancer votre processus de façon isolé via les features de votre kernel (namespaces & co, etc.)
        *** runC can help you avoid being strongly tied to specific technologies, hardware, or cloud service providers.

    ** *containerd* et *runc* ont commencé à apparaître à partir de Docker *1.11.0* (2016/04) ? +
        à confirmer via https://jvns.ca/blog/2016/10/02/i-just-want-to-run-a-container/ (site de 2016)
            **** OUI, confirmé via https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426 : +
            "Docker Engine 1.11 was the first release built on runC (a runtime based on Open Container Intiative technology) and containerd."

    ** détailler ici les low-level container runtimes, et les high-level container runtimes

5. *l'arrivée de Kubernetes, et la démultiplication des runtimes*

    Kubernetes : fait naturellement tourner des containers dans des pods.

    * Donc l'ecosystem des containers est loin de se limiter au seul "Docker", c'est vraiment un *assemblage de diverses technos*, parmi lesquelles on peut citer : 
        ** pour builder des images OCI compliant : Kaniko (Google), buildah (RedHat), Makisu (Uber)
        ** pour lancer des containers depuis des images : CRI-o, rkt, containerd, Kata containers, gVisor, singularity, nabla, podman

    * Parler des confusions possible entre les différents "shim" : le deprecated docker-shim, et containerd-shim

6. *Et... demain ?*
    ** Voir si tente de se projeter dans l'avenir





